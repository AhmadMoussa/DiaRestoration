"""
<Slide Photo Digitalization Software>
    Copyright (C) 2020  Ahmad Yunis Moussa

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

# -*- coding: utf-8 -*-
"""DiaRestore.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mlh4vuGMpwBGYO1ctWWzHjThZ7T-8jMx

First we need to install tensorflow with gpu support
"""

!pip install tensorflow-gpu

"""We need to connect to google drive to access our dataset files."""

from google.colab import drive
drive.mount('/content/gdrive')

!ls "/content/gdrive/My Drive/DiaRestore/"

"""The next snippet is the dataloader class. The main part is the `load_batch` function which allows us to load in arbitrarily sized batches of data to train on."""

import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
import math

import matplotlib.pyplot as plt

class DataLoader():
    def __init__(self, imgs_path, labels_path, batch_size = 4):
        self.imgs_path = imgs_path
        self.labels_path = labels_path
        self.batch_size = batch_size
        self.data, self.labels = self.pre_load_data()
        print(self.data.shape)

    def pre_load_data(self):
        imgs = np.load(self.imgs_path)['arr_0'].astype(dtype=float)
        labels = np.load(self.labels_path)['arr_0'].astype(dtype=float)
        
        return imgs, labels

    def __len__(self):
        '''
        :return:    number of total batches, depends on batch size and index
        '''
        return int(np.floor(len(self.data) / float(self.batch_size)))

    def shuffle_in_unison_scary(self, a, b):
        rng_state = np.random.get_state()
        np.random.shuffle(a)
        np.random.set_state(rng_state)
        np.random.shuffle(b)

    def rotate_image(self, image, angle):
        image_center = tuple(np.array(image.shape[1::-1]) / 2)
        rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)
        result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)
        return result

    def rotate_point(self, point, origin, angle):
      ox, oy = origin
      px, py = point
      qx = ox + math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)
      qy = oy + math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)
      return qx, qy

    def load_batch(self):
        self.shuffle_in_unison_scary(self.data, self.labels)
        for i in range(self.__len__()):
            batch = self.data[i * self.batch_size:(i + 1) * self.batch_size].astype(dtype=float)
            target = self.labels[i * self.batch_size:(i + 1) * self.batch_size].astype(dtype=float)

            for j in range(self.batch_size):


                # shift image a little as data augmentation
                rows, cols, c = batch[j].shape
                sav = np.random.randint(-65, 65)
                sah = np.random.randint(-65, 65)
                M = np.float32([[1, 0, sav], [0, 1, sah]])
                img = cv2.warpAffine(batch[j] , M, (cols, rows))

                angle = np.random.randint(0, 1)
                img = self.rotate_image(img, angle)

                batch[j,] = img / 255.0
 
                # also need to shift
                flat_list = np.array([target[j]])

                flat_list[0][0] = flat_list[0][0] + sav
                flat_list[0][2] = flat_list[0][2] + sav
                flat_list[0][4] = flat_list[0][4] + sav
                flat_list[0][6] = flat_list[0][6] + sav

                flat_list[0][1] = flat_list[0][1] + sah
                flat_list[0][3] = flat_list[0][3] + sah
                flat_list[0][5] = flat_list[0][5] + sah
                flat_list[0][7] = flat_list[0][7] + sah

                origin = tuple(np.array(img.shape[1::-1]) / 2)
                flat_list[0][0], flat_list[0][1] = self.rotate_point((flat_list[0][0], flat_list[0][1]), origin, math.radians(-angle))
                flat_list[0][2], flat_list[0][3] = self.rotate_point((flat_list[0][2], flat_list[0][3]), origin, math.radians(-angle))
                flat_list[0][4], flat_list[0][5] = self.rotate_point((flat_list[0][4], flat_list[0][5]), origin, math.radians(-angle))
                flat_list[0][6], flat_list[0][7] = self.rotate_point((flat_list[0][6], flat_list[0][7]), origin, math.radians(-angle))
                flat_list = flat_list[0]
                target[j,] = np.array(flat_list)

            yield batch, target

"""Next up we need to instantiate the DataLoader Object. We will create one for training and another one for testing:"""

path = "/content/gdrive/My Drive/DiaRestore/"
train_loader = DataLoader(path + "imgs_np.npz", path + "labels_np.npz", batch_size = 32)
test_loader = DataLoader(path + "imgs_test_np.npz", path + "labels_test_np.npz", batch_size = 1)
eval_loader = DataLoader(path + "imgs_test_np.npz", path + "labels_test_np.npz", batch_size = 1)

import matplotlib
from matplotlib import cm
from matplotlib.patches import Polygon
from matplotlib.collections import PatchCollection
import PIL.ImageDraw as ImageDraw
import PIL.Image as Image

for j, (imgs, labels) in enumerate(train_loader.load_batch()):
    labels = labels[0]
    #labels = [labels[i:i + 2] for i in range(0, len(labels), 2)]
    print(labels)
    
    img = Image.fromarray(np.uint8(imgs[0]*255))
    print(img.size)
    draw = ImageDraw.Draw(img)

    # points = ((1,1), (2,1), (2,2), (1,2), (0.5,1.5))
    print(np.array(labels).flatten().tolist())
    draw.polygon(np.array(labels).flatten().tolist(), fill = "wheat")

    display(img)

    break

# test the archive
imgs = np.load(path + "imgs_np.npz")
import matplotlib.pyplot as plt
plt.imshow(imgs['arr_0'][0])

"""Now we need to create our model."""

import keras
from keras.models import Model
from keras.layers import Input, Conv2D, Dense, Flatten

net_in = Input(shape=(504,378,3))
conv1 = Conv2D(16,7, activation="relu")(net_in)
conv12 = Conv2D(16,7, strides=(2,2), activation="relu")(conv1)


conv2 = Conv2D(48,3, activation="relu")(conv12)
conv22 = Conv2D(48,3, strides=(2,2), activation="relu")(conv2)


conv3 = Conv2D(64,3, activation="relu")(conv22)
conv32 = Conv2D(64,3, strides=(2,2), activation="relu")(conv3)


conv4 = Conv2D(96,3, strides = (2,2), activation="relu")(conv32)
conv42 = Conv2D(128,3, strides=(2,2), activation="relu")(conv4)

flat = Flatten()(conv42)
dense = Dense(8)(flat)

model = Model(net_in, dense)
model.summary()

"""Next we should choose an optimizer and compile the model:"""

import keras.backend as K
def root_mean_squared_error(y_true, y_pred):
    return K.sqrt(K.mean(K.square(y_pred - y_true)))

from keras.optimizers import Adam

model.compile(optimizer=Adam(lr = 0.002), loss=root_mean_squared_error, loss_weights = [1], metrics=["accuracy"])

"""And finally we can define the training loop"""

train_loss_hist = []
eval_loss_hist = []
plt.ylim(top = 250)
epochs = 1000
for e in range(epochs):
    #plt.plot(train_loss_hist)
    #plt.plot(eval_loss_hist)
    #plt.show()
    #plt.close('all')

    if e % 5 == 0:
      model.save("/content/gdrive/My Drive/DiaRestore/model_{}".format(e))
      np.save("/content/gdrive/My Drive/DiaRestore/train_hist.npy",train_loss_hist)
      np.save("/content/gdrive/My Drive/DiaRestoreeval_hist.npy",eval_loss_hist)
      
    if e % 5 == 0:
        for k, (test_imgs, test_labels) in enumerate(test_loader.load_batch()):
            prediction = model.predict(test_imgs)
            to_show = test_imgs[0] * 255

            prediction = prediction[0]
            img = Image.fromarray(np.uint8(test_imgs[0]*255))

            draw = ImageDraw.Draw(img)
            draw.polygon(np.array(prediction).flatten().tolist(), fill = "wheat")

            display(img)
            
            
            #cv2.imwrite("/content/gdrive/My Drive/DiaRestore/imgs/img_{}_{}.png".format(e, k), to_show)
            
    for j, (eval_img, eval_label) in enumerate(eval_loader.load_batch()):
        eval_loss = model.test_on_batch(eval_img, eval_label)
        eval_loss_hist.append(eval_loss[0])
        #print("[E: {}/{}][B: {}/{}][Loss: {}]".format(e, epochs, j, len(train_loader), loss))

    for j, (imgs, labels) in enumerate(train_loader.load_batch()):
        train_loss = model.train_on_batch(imgs, labels)
        train_loss_hist.append(train_loss[0])
        
        
        print("[E: {}/{}][B: {}/{}][Loss: {}]".format(e, epochs, j, len(train_loader), train_loss))